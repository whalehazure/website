# UyPro æ–°é—»çˆ¬è™«ç³»ç»Ÿ API æ–‡æ¡£

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†UyProæ–°é—»çˆ¬è™«ç³»ç»Ÿçš„APIæ¥å£ã€å‡½æ•°è¯´æ˜å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ¨¡å—

### 1. webmod.py - ç½‘ç«™è§£ææ¨¡å—

#### æ ¸å¿ƒå‡½æ•°

##### `parsetweet()`
```python
def parsetweet(item, article_title, article_content, tweet_author, tweet_createtime, 
               tweet_img_url, html_content, dt="America/New_York", split_func=split_string, 
               translate=True, max_length=4800, _translatetext=translatetext, 
               convert_traditional=False)
```

**åŠŸèƒ½**: ç»Ÿä¸€çš„æ–°é—»æ–‡ç« æ•°æ®å¤„ç†å’Œç¿»è¯‘å‡½æ•°

**å‚æ•°**:
- `item` (UyproItem): æ•°æ®é¡¹å¯¹è±¡
- `article_title` (str): æ–‡ç« æ ‡é¢˜
- `article_content` (str): æ–‡ç« å†…å®¹
- `tweet_author` (str): ä½œè€…ä¿¡æ¯
- `tweet_createtime` (str): å‘å¸ƒæ—¶é—´
- `tweet_img_url` (list): å›¾ç‰‡URLåˆ—è¡¨
- `html_content` (str): åŸå§‹HTML
- `dt` (str): æ—¶åŒºè®¾ç½®
- `split_func` (function): æ–‡æœ¬åˆ†å‰²å‡½æ•°
- `translate` (bool): æ˜¯å¦ç¿»è¯‘
- `max_length` (int): æœ€å¤§ç¿»è¯‘é•¿åº¦
- `_translatetext` (function): ç¿»è¯‘å‡½æ•°
- `convert_traditional` (bool): æ˜¯å¦è½¬æ¢ç¹ä½“ä¸­æ–‡

**è¿”å›å€¼**: 
- `UyproItem`: å¤„ç†å®Œæˆçš„æ•°æ®é¡¹
- `None`: å¤„ç†å¤±è´¥

#### ç½‘ç«™è§£æå‡½æ•°

##### `parse_tweet_jpost(response, item)`
**åŠŸèƒ½**: Jerusalem Postç½‘ç«™è§£æ
**ç‰¹ç‚¹**: 
- æ”¯æŒå®¢æˆ·ç«¯æ¸²æŸ“é¡µé¢
- å¤„ç†JavaScriptä¸­çš„JSON-LDæ•°æ®
- å¤šé‡è½¬ä¹‰å­—ç¬¦å¤„ç†

##### `parse_tweet_bbc(response, item)`
**åŠŸèƒ½**: BBCæ–°é—»ç½‘ç«™è§£æ
**ç‰¹ç‚¹**:
- æ ‡å‡†HTMLç»“æ„è§£æ
- å¤šåª’ä½“å†…å®¹æå–
- æ—¶é—´æ ¼å¼æ ‡å‡†åŒ–

##### `parse_tweet_scmp(response, item)`
**åŠŸèƒ½**: South China Morning Postè§£æ
**ç‰¹ç‚¹**:
- ä»˜è´¹å¢™å†…å®¹å¤„ç†
- å›¾ç‰‡æ‡’åŠ è½½å¤„ç†
- ä½œè€…ä¿¡æ¯æå–

### 2. utils.py - å·¥å…·å‡½æ•°æ¨¡å—

#### æ—¥æœŸæ—¶é—´å¤„ç†

##### `parse_date(date_input, default_timezone="America/New_York")`
```python
def parse_date(date_input, default_timezone="America/New_York")
```

**åŠŸèƒ½**: è§£æå„ç§æ ¼å¼çš„æ—¥æœŸæ—¶é—´

**æ”¯æŒæ ¼å¼**:
- Unixæ—¶é—´æˆ³
- ISOæ ¼å¼æ—¥æœŸ
- æ³¢æ–¯å†æ—¥æœŸ
- ä¸­æ–‡æ—¥æœŸæ ¼å¼
- ä¿„è¯­æœˆä»½åç§°

**å‚æ•°**:
- `date_input` (str|int): è¾“å…¥æ—¥æœŸ
- `default_timezone` (str): é»˜è®¤æ—¶åŒº

**è¿”å›å€¼**: 
- `str`: æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸² "YYYY-MM-DD HH:MM:SS"

#### ç¿»è¯‘æœåŠ¡

##### `translatetext(text, target_lang='zh')`
**åŠŸèƒ½**: Googleç¿»è¯‘æœåŠ¡
**å‚æ•°**:
- `text` (str): å¾…ç¿»è¯‘æ–‡æœ¬
- `target_lang` (str): ç›®æ ‡è¯­è¨€

##### `translatetext_bing(text, target_lang='zh')`
**åŠŸèƒ½**: Bingç¿»è¯‘æœåŠ¡

##### `translate_text_gemini(text, target_lang='zh')`
**åŠŸèƒ½**: Google Geminiç¿»è¯‘æœåŠ¡

##### `translate_text_siliconflow(text, target_lang='zh')`
**åŠŸèƒ½**: SiliconFlowç¿»è¯‘æœåŠ¡

#### æ–‡æœ¬å¤„ç†

##### `split_string(text, max_length=4800)`
**åŠŸèƒ½**: æ™ºèƒ½æ–‡æœ¬åˆ†å‰²
**ç‰¹ç‚¹**:
- æŒ‰å¥å­è¾¹ç•Œåˆ†å‰²
- ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- æ”¯æŒå¤šè¯­è¨€

##### `remove_font_tags(html_content)`
**åŠŸèƒ½**: æ¸…ç†HTMLå­—ä½“æ ‡ç­¾

##### `detect_language(text)`
**åŠŸèƒ½**: è‡ªåŠ¨è¯­è¨€æ£€æµ‹

### 3. items.py - æ•°æ®é¡¹å®šä¹‰

#### UyproItemç±»

```python
class UyproItem(scrapy.Item):
    # åŸºç¡€ä¿¡æ¯
    ch_url = scrapy.Field()              # é¢‘é“URL
    tweet_url = scrapy.Field()           # æ–‡ç« URL
    tweet_id = scrapy.Field()            # æ–‡ç« ID
    
    # å†…å®¹å­—æ®µ
    tweet_title = scrapy.Field()         # åŸå§‹æ ‡é¢˜
    tweet_title_tslt = scrapy.Field()    # ç¿»è¯‘æ ‡é¢˜
    tweet_content = scrapy.Field()       # åŸå§‹å†…å®¹
    tweet_content_tslt = scrapy.Field()  # ç¿»è¯‘å†…å®¹
    tweet_author = scrapy.Field()        # ä½œè€…
    tweet_lang = scrapy.Field()          # è¯­è¨€
    
    # æ—¶é—´å­—æ®µ
    tweet_createtime = scrapy.Field()    # æ ‡å‡†åŒ–æ—¶é—´
    tweet_createtime_original = scrapy.Field()  # åŸå§‹æ—¶é—´
    
    # åª’ä½“æ–‡ä»¶
    tweet_img_url = scrapy.Field()       # å›¾ç‰‡URLåˆ—è¡¨
    tweet_pdf_url = scrapy.Field()       # PDF URLåˆ—è¡¨
    tweet_table = scrapy.Field()         # è¡¨æ ¼æ•°æ®
    
    # ç³»ç»Ÿå­—æ®µ
    taskid = scrapy.Field()              # ä»»åŠ¡ID
    deviceid = scrapy.Field()            # è®¾å¤‡ID
    bid = scrapy.Field()                 # æ‰¹æ¬¡ID
    capture_time = scrapy.Field()        # é‡‡é›†æ—¶é—´
```

## ğŸ”§ é…ç½®è¯´æ˜

### settings.py ä¸»è¦é…ç½®

#### åŸºç¡€è®¾ç½®
```python
BOT_NAME = "uyPro"                    # çˆ¬è™«åç§°
USER_AGENT = "Mozilla/5.0..."         # ç”¨æˆ·ä»£ç†
ROBOTSTXT_OBEY = False                # å¿½ç•¥robots.txt
```

#### å¹¶å‘è®¾ç½®
```python
CONCURRENT_REQUESTS = 32              # å¹¶å‘è¯·æ±‚æ•°
DOWNLOAD_DELAY = 3                    # ä¸‹è½½å»¶è¿Ÿ
CONCURRENT_REQUESTS_PER_DOMAIN = 16   # æ¯åŸŸåå¹¶å‘æ•°
```

#### ä¸­é—´ä»¶é…ç½®
```python
DOWNLOADER_MIDDLEWARES = {
    "uyPro.middlewares.UyproDownloaderMiddleware": 543,
}
```

#### è¶…æ—¶è®¾ç½®
```python
CLOSESPIDER_TIMEOUT_NO_ITEM = 1800    # æ— æ•°æ®è¶…æ—¶(30åˆ†é’Ÿ)
CLOSESPIDER_ITEMCOUNT = 300           # æœ€å¤§çˆ¬å–æ•°é‡
CLOSESPIDER_TIMEOUT = 7200            # æ€»è¶…æ—¶æ—¶é—´(2å°æ—¶)
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬çˆ¬è™«è¿è¡Œ

```python
# è¿è¡Œå•ä¸ªçˆ¬è™«
scrapy crawl jpost

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
python run_spiders.py
```

### è‡ªå®šä¹‰è§£æå‡½æ•°

```python
def parse_tweet_custom(response, item):
    """è‡ªå®šä¹‰ç½‘ç«™è§£æå‡½æ•°"""
    # æå–æ ‡é¢˜
    title = response.xpath("//h1/text()").get('')
    
    # æå–å†…å®¹
    content_nodes = response.xpath("//div[@class='content']//p")
    content = '\n'.join([p.xpath('string(.)').get('') for p in content_nodes])
    
    # æå–ä½œè€…
    author = response.xpath("//span[@class='author']/text()").get('')
    
    # æå–æ—¶é—´
    time_str = response.xpath("//time/@datetime").get('')
    
    # æå–å›¾ç‰‡
    images = response.xpath("//img/@src").getall()
    
    # è·å–HTMLå†…å®¹
    html_content = response.xpath("//div[@class='content']").get('')
    
    # è°ƒç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    return parsetweet(item, title, content, author, time_str, images, html_content)
```

### ç¿»è¯‘æœåŠ¡ä½¿ç”¨

```python
from uyPro.spiders.utils import translatetext, translate_text_gemini

# ä½¿ç”¨Googleç¿»è¯‘
result = translatetext("Hello World", target_lang='zh')

# ä½¿ç”¨Geminiç¿»è¯‘
result = translate_text_gemini("Hello World", target_lang='zh')
```

## ğŸ“Š æ•°æ®æµç¨‹

```
1. çˆ¬è™«å¯åŠ¨ â†’ 2. è·å–é¡µé¢ â†’ 3. è§£æå†…å®¹ â†’ 4. æ•°æ®å¤„ç† â†’ 5. ç¿»è¯‘æœåŠ¡ â†’ 6. å­˜å‚¨æ•°æ®
     â†“              â†“              â†“              â†“              â†“              â†“
  Spider        Response      parse_tweet_*    parsetweet    translatetext    Redis/File
```

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### æ—¥å¿—é…ç½®
- `website.log` - ä¸»è¦è¿è¡Œæ—¥å¿—
- `task.log` - ä»»åŠ¡æ‰§è¡Œæ—¥å¿—
- `watchdog_scrapy.log` - ç›‘æ§æ—¥å¿—

### å¸¸ç”¨è°ƒè¯•å‘½ä»¤
```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f website.log

# æ£€æŸ¥RedisçŠ¶æ€
redis-cli ping

# æµ‹è¯•å•ä¸ªURL
scrapy shell "https://example.com"
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä»£ç†é…ç½®**: ç¡®ä¿ä»£ç†åˆ—è¡¨æœ‰æ•ˆä¸”å¯ç”¨
2. **ç¿»è¯‘é™åˆ¶**: æ³¨æ„å„ç¿»è¯‘æœåŠ¡çš„APIé™åˆ¶
3. **å†…å­˜ç®¡ç†**: å¤§é‡æ•°æ®å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨
4. **é”™è¯¯å¤„ç†**: å®ç°é€‚å½“çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
5. **æ—¶åŒºå¤„ç†**: æ­£ç¡®è®¾ç½®å„ç½‘ç«™çš„æ—¶åŒºä¿¡æ¯

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–éœ€è¦æŠ€æœ¯æ”¯æŒï¼Œè¯·æŸ¥çœ‹ï¼š
- é¡¹ç›®README.md
- æ—¥å¿—æ–‡ä»¶åˆ†æ
