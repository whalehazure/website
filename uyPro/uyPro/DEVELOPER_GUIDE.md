# UyPro å¼€å‘è€…æŒ‡å—

## ğŸ¯ å¼€å‘ç¯å¢ƒæ­å»º

### 1. ç¯å¢ƒè¦æ±‚
- Python 3.8+
- Redis 6.0+
- Google Chrome + ChromeDriver
- Git

### 2. é¡¹ç›®å…‹éš†å’Œå®‰è£…
```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd uyPro

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 3. é…ç½®æ–‡ä»¶è®¾ç½®
```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp configlinux.ini.template configlinux.ini  # Linux
cp configwin.ini.template configwin.ini      # Windows

# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim configlinux.ini
```

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### ç›®å½•ç»“æ„
```
uyPro/
â”œâ”€â”€ uyPro/                    # ä¸»é¡¹ç›®ç›®å½•
â”‚   â”œâ”€â”€ spiders/             # çˆ¬è™«æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ webmod.py        # æ ¸å¿ƒè§£ææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ utils.py         # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ [ç½‘ç«™].py        # å„ç½‘ç«™çˆ¬è™«
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py             # æ•°æ®é¡¹å®šä¹‰
â”‚   â”œâ”€â”€ pipelines.py         # æ•°æ®å¤„ç†ç®¡é“
â”‚   â”œâ”€â”€ middlewares.py       # ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ settings.py          # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ proxy.list               # ä»£ç†åˆ—è¡¨
â”œâ”€â”€ configlinux.ini          # Linuxé…ç½®
â”œâ”€â”€ configwin.ini            # Windowsé…ç½®
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ scrapy.cfg              # Scrapyé…ç½®
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜
```

### æ ¸å¿ƒç»„ä»¶

#### 1. çˆ¬è™«å¼•æ“ (Spider Engine)
- **ä½ç½®**: `uyPro/spiders/`
- **åŠŸèƒ½**: ç½‘ç«™çˆ¬å–å’Œé¡µé¢è§£æ
- **ä¸»è¦æ–‡ä»¶**: å„ç½‘ç«™çš„çˆ¬è™«æ–‡ä»¶

#### 2. è§£æå¼•æ“ (Parser Engine)
- **ä½ç½®**: `uyPro/spiders/webmod.py`
- **åŠŸèƒ½**: ç»Ÿä¸€çš„å†…å®¹è§£æå’Œå¤„ç†
- **æ ¸å¿ƒå‡½æ•°**: `parsetweet()`, `parse_tweet_**()`

#### 3. ç¿»è¯‘å¼•æ“ (Translation Engine)
- **ä½ç½®**: `uyPro/spiders/utils.py`
- **åŠŸèƒ½**: å¤šå¼•æ“ç¿»è¯‘æœåŠ¡
- **æ”¯æŒå¼•æ“**: Google, Bing, Gemini, SiliconFlow

#### 4. æ•°æ®ç®¡é“ (Data Pipeline)
- **ä½ç½®**: `uyPro/pipelines.py`
- **åŠŸèƒ½**: æ•°æ®æ¸…æ´—ã€å­˜å‚¨ã€åå¤„ç†

## ğŸ”§ æ·»åŠ æ–°ç½‘ç«™æ”¯æŒ

### æ­¥éª¤1: åˆ›å»ºçˆ¬è™«æ–‡ä»¶

```python
# uyPro/spiders/newsite.py
import scrapy
from uyPro.items import UyproItem
from .webmod import parse_tweet_newsite

class NewsiteSpider(scrapy.Spider):
    name = 'newsite'
    allowed_domains = ['newsite.com']
    
    def start_requests(self):
        urls = [
            'https://newsite.com/news',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
    
    def parse(self, response):
        # æå–æ–‡ç« é“¾æ¥
        article_links = response.xpath("//a[@class='article-link']/@href").getall()
        
        for link in article_links:
            yield response.follow(link, self.parse_article)
    
    def parse_article(self, response):
        item = UyproItem()
        item['ch_url'] = 'https://newsite.com/news'
        item['tweet_url'] = response.url
        item['tweet_id'] = response.url
        item['taskid'] = 'newsite_task'
        item['bid'] = 'newsite_bid'
        item['tweet_lang'] = 'en'
        
        # è°ƒç”¨è§£æå‡½æ•°
        return parse_tweet_newsite(response, item)
```

### æ­¥éª¤2: æ·»åŠ è§£æå‡½æ•°

```python
# åœ¨ uyPro/spiders/webmod.py ä¸­æ·»åŠ 
def parse_tweet_newsite(response, item):
    """
    æ–°ç½‘ç«™è§£æå‡½æ•°
    
    Args:
        response: Scrapyå“åº”å¯¹è±¡
        item: æ•°æ®é¡¹å¯¹è±¡
    
    Returns:
        å¤„ç†åçš„æ•°æ®é¡¹
    """
    # æå–æ ‡é¢˜
    article_title = response.xpath("//h1[@class='title']/text()").get('').strip()
    
    # æå–å†…å®¹
    content_nodes = response.xpath("//div[@class='content']//p")
    article_content = '\n'.join([
        p.xpath('string(.)').get('').strip() 
        for p in content_nodes if p
    ]).strip()
    
    # æå–ä½œè€…
    tweet_author = response.xpath("//span[@class='author']/text()").get('').strip()
    
    # æå–æ—¶é—´
    tweet_createtime = response.xpath("//time/@datetime").get('').strip()
    
    # æå–å›¾ç‰‡
    img_url = response.xpath("//img/@src").getall()
    
    # è·å–HTMLå†…å®¹
    html_content = response.xpath("//div[@class='content']").get('')
    
    # è°ƒç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    return parsetweet(item, article_title, article_content, tweet_author, 
                     tweet_createtime, img_url, html_content, dt="UTC")
```

### æ­¥éª¤3: æ³¨å†Œçˆ¬è™«

åœ¨ `spider_mapping.json` ä¸­æ·»åŠ æ–°çˆ¬è™«ï¼š
```json
{
    "newsite": {
        "name": "newsite",
        "description": "æ–°ç½‘ç«™çˆ¬è™«",
        "domain": "newsite.com",
        "language": "en",
        "timezone": "UTC"
    }
}
```

## ğŸ§ª æµ‹è¯•å’Œè°ƒè¯•

### 1. å•å…ƒæµ‹è¯•

```python
# tests/test_newsite.py
import unittest
from scrapy.http import HtmlResponse
from uyPro.items import UyproItem
from uyPro.spiders.webmod import parse_tweet_newsite

class TestNewsiteParser(unittest.TestCase):
    def setUp(self):
        self.html_content = """
        <html>
            <h1 class="title">Test Title</h1>
            <div class="content">
                <p>Test content paragraph 1</p>
                <p>Test content paragraph 2</p>
            </div>
            <span class="author">Test Author</span>
            <time datetime="2024-01-15T10:30:00Z">Jan 15, 2024</time>
        </html>
        """
        
    def test_parse_article(self):
        response = HtmlResponse(
            url='https://newsite.com/article/123',
            body=self.html_content.encode('utf-8')
        )
        item = UyproItem()
        
        result = parse_tweet_newsite(response, item)
        
        self.assertEqual(result['tweet_title'], 'Test Title')
        self.assertIn('Test content', result['tweet_content'])
        self.assertEqual(result['tweet_author'], 'Test Author')

if __name__ == '__main__':
    unittest.main()
```

### 2. è°ƒè¯•æŠ€å·§

#### ä½¿ç”¨Scrapy Shell
```bash
# å¯åŠ¨Scrapy Shell
scrapy shell "https://newsite.com/article/123"

# åœ¨Shellä¸­æµ‹è¯•XPath
>>> response.xpath("//h1[@class='title']/text()").get()
'Test Title'

>>> response.xpath("//div[@class='content']//p/text()").getall()
['Test content paragraph 1', 'Test content paragraph 2']
```

#### æ—¥å¿—è°ƒè¯•
```python
import logging

def parse_tweet_newsite(response, item):
    logging.info(f"è§£æURL: {response.url}")
    
    title = response.xpath("//h1[@class='title']/text()").get('').strip()
    logging.info(f"æå–æ ‡é¢˜: {title}")
    
    if not title:
        logging.warning("æœªæ‰¾åˆ°æ ‡é¢˜")
        return None
    
    # ... å…¶ä»–è§£æé€»è¾‘
```

## ğŸ”„ ç¿»è¯‘å¼•æ“æ‰©å±•

### æ·»åŠ æ–°ç¿»è¯‘å¼•æ“

```python
# åœ¨ uyPro/spiders/utils.py ä¸­æ·»åŠ 
def translate_text_newengine(text, target_lang='zh'):
    """
    æ–°ç¿»è¯‘å¼•æ“å‡½æ•°
    
    Args:
        text: å¾…ç¿»è¯‘æ–‡æœ¬
        target_lang: ç›®æ ‡è¯­è¨€
    
    Returns:
        ç¿»è¯‘ç»“æœ
    """
    try:
        # å®ç°ç¿»è¯‘é€»è¾‘
        api_url = "https://api.newengine.com/translate"
        payload = {
            'text': text,
            'target': target_lang,
            'source': 'auto'
        }
        
        response = requests.post(api_url, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        return result.get('translated_text', text)
        
    except Exception as e:
        logging.error(f"æ–°ç¿»è¯‘å¼•æ“é”™è¯¯: {e}")
        return text
```

### é›†æˆåˆ°ä¸»ç¿»è¯‘å‡½æ•°

```python
def translatetext_unified(text, target_lang='zh', engine='auto'):
    """
    ç»Ÿä¸€ç¿»è¯‘æ¥å£
    
    Args:
        text: å¾…ç¿»è¯‘æ–‡æœ¬
        target_lang: ç›®æ ‡è¯­è¨€
        engine: ç¿»è¯‘å¼•æ“ ('google', 'bing', 'gemini', 'siliconflow', 'newengine', 'auto')
    
    Returns:
        ç¿»è¯‘ç»“æœ
    """
    engines = {
        'google': translatetext,
        'bing': translatetext_bing,
        'gemini': translate_text_gemini,
        'siliconflow': translate_text_siliconflow,
        'newengine': translate_text_newengine,
    }
    
    if engine == 'auto':
        # è‡ªåŠ¨é€‰æ‹©å¯ç”¨å¼•æ“
        for engine_name, engine_func in engines.items():
            try:
                result = engine_func(text, target_lang)
                if result and result != text:
                    return result
            except:
                continue
        return text
    
    engine_func = engines.get(engine, translatetext)
    return engine_func(text, target_lang)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘ä¼˜åŒ–
```python
# settings.py
CONCURRENT_REQUESTS = 32
CONCURRENT_REQUESTS_PER_DOMAIN = 8
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = 0.5
```

### 2. å†…å­˜ä¼˜åŒ–
```python
# åœ¨è§£æå‡½æ•°ä¸­åŠæ—¶æ¸…ç†å¤§å¯¹è±¡
def parse_tweet_large_content(response, item):
    # å¤„ç†å¤§é‡æ•°æ®æ—¶
    content = response.xpath("//div[@class='content']").get('')
    
    # å¤„ç†å®Œæˆåæ¸…ç†
    del content
    
    return result
```

### 3. ç¼“å­˜ä¼˜åŒ–
```python
# settings.py
HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 3600  # 1å°æ—¶ç¼“å­˜
HTTPCACHE_DIR = 'httpcache'
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®
```bash
# å®‰è£…ç”Ÿäº§ä¾èµ–
pip install gunicorn supervisor

# é…ç½®Supervisor
sudo vim /etc/supervisor/conf.d/uyPro.conf
```

### 2. Dockeréƒ¨ç½²

```dockerfile
FROM python:3.8-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY .. .
CMD ["python", "run_spiders.py"]
```

### 3. ç›‘æ§å’Œæ—¥å¿—
```python
# é…ç½®æ—¥å¿—è½®è½¬
LOGGING = {
    'version': 1,
    'handlers': {
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'uyPro.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
        },
    },
}
```

## ğŸ“ ä»£ç è§„èŒƒ

### 1. å‘½åè§„èŒƒ
- å‡½æ•°å: `snake_case`
- ç±»å: `PascalCase`
- å¸¸é‡: `UPPER_CASE`
- å˜é‡: `snake_case`

### 2. æ–‡æ¡£å­—ç¬¦ä¸²
```python
def parse_tweet_example(response, item):
    """
    ç¤ºä¾‹ç½‘ç«™è§£æå‡½æ•°
    
    Args:
        response (HtmlResponse): Scrapyå“åº”å¯¹è±¡
        item (UyproItem): æ•°æ®é¡¹å¯¹è±¡
    
    Returns:
        UyproItem: å¤„ç†åçš„æ•°æ®é¡¹
        None: è§£æå¤±è´¥æ—¶è¿”å›
    
    Raises:
        ValueError: å½“å¿…è¦å­—æ®µç¼ºå¤±æ—¶
    """
    pass
```

### 3. é”™è¯¯å¤„ç†
```python
def safe_extract(response, xpath, default=''):
    """å®‰å…¨çš„æ•°æ®æå–"""
    try:
        return response.xpath(xpath).get(default).strip()
    except Exception as e:
        logging.error(f"æå–å¤±è´¥ {xpath}: {e}")
        return default
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/new-site`
3. æäº¤æ›´æ”¹: `git commit -am 'Add new site support'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/new-site`
5. åˆ›å»ºPull Request

## ğŸ“ è·å–å¸®åŠ©

- æŸ¥çœ‹é¡¹ç›®Wiki
- æäº¤GitHub Issue
- å‚è€ƒAPIæ–‡æ¡£
- æŸ¥çœ‹ç¤ºä¾‹ä»£ç 
